# Spark Configuration for Iceberg with AWS

# Iceberg Catalog Configuration
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.iceberg_catalog=org.apache.iceberg.spark.SparkCatalog

# ==============================================================================
# Option 1: AWS S3 Tables (Recommended for new projects)
# ==============================================================================
# Use this configuration for AWS S3 Tables with managed table buckets
spark.sql.catalog.iceberg_catalog.catalog-impl=org.apache.iceberg.aws.s3.S3TablesCatalog
spark.sql.catalog.iceberg_catalog.table-bucket-arn=arn:aws:s3tables:us-east-1:123456789012:bucket/your-table-bucket
spark.sql.catalog.iceberg_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO

# ==============================================================================
# Option 2: AWS Glue Data Catalog (Classic approach)
# ==============================================================================
# Uncomment these lines to use AWS Glue instead of S3 Tables
# spark.sql.catalog.iceberg_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog
# spark.sql.catalog.iceberg_catalog.warehouse=s3a://your-bucket/warehouse
# spark.sql.catalog.iceberg_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO

# AWS S3 Configuration
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem

# AWS Credentials Configuration
# Option 1: Use default credentials provider chain (recommended for production)
spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain

# Option 2: Use explicit access keys (set via environment variables)
# Uncomment these lines to use explicit credentials instead of default provider chain
# spark.hadoop.fs.s3a.access.key=${AWS_ACCESS_KEY_ID}
# spark.hadoop.fs.s3a.secret.key=${AWS_SECRET_ACCESS_KEY}
# spark.hadoop.fs.s3a.session.token=${AWS_SESSION_TOKEN}  # Only for temporary credentials

# Optional: S3 Endpoint Configuration
# spark.hadoop.fs.s3a.endpoint=s3.amazonaws.com
# spark.hadoop.fs.s3a.path.style.access=false

# Optional: S3 Performance Tuning
# spark.hadoop.fs.s3a.connection.maximum=100
# spark.hadoop.fs.s3a.connection.timeout=200000
# spark.hadoop.fs.s3a.threads.max=20
# spark.hadoop.fs.s3a.fast.upload=true

# Optional: Enable S3 Server-Side Encryption
# spark.hadoop.fs.s3a.server-side-encryption-algorithm=AES256

# Spark Application Configuration
spark.app.name=Spark Iceberg AWS Application
spark.driver.memory=4g
spark.executor.memory=4g
spark.executor.cores=2
spark.dynamicAllocation.enabled=true

# Iceberg Table Properties
spark.sql.catalog.iceberg_catalog.table-default.write.format.default=parquet
spark.sql.catalog.iceberg_catalog.table-default.write.parquet.compression-codec=zstd

# Optional: Configure AWS Region
# spark.hadoop.fs.s3a.endpoint.region=us-east-1
